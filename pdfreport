Practical Machine Learning Project Title: “Quantified Self Movement Report” author: “By Jaja Yogo” date: “Thursday, August 20, 2015” output: word_document — Executive Summary Utilizing devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These form of devices are part of the quantified self-movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. The goal of this project is to use data from accelerometers on the belt, forearm, arm, and dumbbell of six (6) participants as they perform barbell lifts correctly and incorrectly 5 different ways. GitHub Repo: https: Introduction Six young participants aged between 20-28 years old were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different ways: * Class A - exactly according to the specification * Class B - throwing the elbows to the front * Class C - lifting the dumbbell only halfway * Class D - lowering the dumbbell only halfway * Class E - throwing the hips to the front Class A corresponds to the specified execution of the exercise, while the other 4 classes correspond to common mistakes. Participants were supervised by an experienced weight lifter to make sure the execution complied with the manner they were supposed to simulate. The Research scientists made sure that all participants could easily simulate the mistakes in a safe and controlled manner by using a relatively light dumbbell (1.25kg). The objective of this report is to predict the manner in which subjects did the exercise. This is the “classe” variable in the training set. The model will use the other variables to predict with. This report describes; (i) how the model is built, (ii) use of cross validation and (iii) an estimate of expected out of sample error. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset). Getting and Cleaning Data library(rpart) library(rpart.plot) library(randomForest) library(corrplot) library(rattle) Load Data
{r}url_raw_training <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"file_dest_training <- "pml-training.csv" #download.file(url=url_raw_training, destfile=file_dest_training, method="curl") url_raw_testing <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv" file_dest_testing <- "pml-testing.csv" #download.file(url=url_raw_testing, destfile=file_dest_testing, method="curl") Import the data treating empty values as NA. We will attempt to get reject observation with missing values (NA) in this step {r}mytraining <- read.csv(file_dest_training, na.strings= c("NA",""), header=TRUE) colnames_train <- colnames(mytraining) mytesting <- read.csv(file_dest_testing, na.strings= c("NA",""), header=TRUE) colnames_test <- colnames(mytesting) Read and Clean the Data After downloading the data from the data source, we can read the two csv files into two data frames. {r}dim(mytraining) dim(mytesting) sum(complete.cases(mytraining)) colnames(mytraining) colnames(mytesting) > dim(mytraining) [1] 19622 160 > dim(mytesting) [1] 20 160 > #clean data > sum(complete.cases(mytraining)) [1] 406 Remove columns with NA missing values {r}mytraining <- mytraining[, colSums(is.na(mytraining)) == 0] mytesting <- mytesting[, colSums(is.na(mytesting)) == 0] Verify that the column names (excluding classe and problem_id) are identical in the training and test set. {r}classe <- mytraining$classe trainRemove <- grepl("^X|timestamp|window", names(mytraining)) mytraining <- mytraining[, !trainRemove] trainCleaned <- mytraining[, sapply(mytraining, is.numeric)] trainCleaned$classe <- classe testRemove <- grepl("^X|timestamp|window", names(mytesting)) mytesting <- mytesting[, !testRemove] testCleaned <- mytesting[, sapply(mytesting, is.numeric)] Data Set Validation The cleaned training data set contains 19622 observations and 53 variables. The testing data set contains 20 observations and 53 variables. The “classe” variable is still in the cleaned training set. Use validation data set for further evaluation. Search for covariates with almost without variability. idsless <- createDataPartition(y=mytraining$classe, p=0.25, list=FALSE) myless1 <- mytraining[idsless,] myremainder <- mytraining[-idsless,]
set.seed(11355) idsless <- createDataPartition(y=myremainder$classe, p=0.33, list=FALSE) myless2 <- myremainder[idsless,] myremainder <- myremainder[-idsless,] set.seed(11355) idsless <- createDataPartition(y=myremainder$classe, p=0.5, list=FALSE) myless3 <- myremainder[idsless,] myless4 <- myremainder[-idsless,] Train Model and Algorith We will trim the data for further analysis. Divide each of these 4 sets into training (60%) and test (40%) sets. {r}set.seed(11355)# For reproducibile purpose inTrain <- createDataPartition(y=myless1$classe, p=0.6, list=FALSE) mylesstraining1 <- myless1[inTrain,] mylesstesting1 <- myless1[-inTrain,] set.seed(11355) inTrain <- createDataPartition(y=myless2$classe, p=0.6, list=FALSE) mylesstraining2 <- myless2[inTrain,] mylesstesting2 <- myless2[-inTrain,] set.seed(11355) inTrain <- createDataPartition(y=myless3$classe, p=0.6, list=FALSE) mylesstraining3 <- myless3[inTrain,] mylesstesting3 <- myless3[-inTrain,] set.seed(11355) inTrain <- createDataPartition(y=myless4$classe, p=0.6, list=FALSE) mylesstraining4 <- myless4[inTrain,] mylesstesting4 <- myless4[-inTrain,] Evaluation of Data We will use classification tree to evaluate data on train on training set 1 of 4 with no extra features. {r}set.seed(11355) modFit <- train(mylesstraining1$classe ~ ., data = mylesstraining1, method="rpart") print(modFit, digits=3) print(modFit$finalModel, digits=3) # See fancyrpartplot in appendix section. CART
2946 samples 53 predictor 5 classes: 'A', 'B', 'C', 'D', 'E'
No pre-processing Resampling: Bootstrapped (25 reps) Summary of sample sizes: 2946, 2946, 2946, 2946, 2946, 2946, ... Resampling results across tuning parameters:
cp Accuracy Kappa Accuracy SD Kappa SD 0.0394 0.500 0.3561 0.0585 0.0961 0.0542 0.397 0.1848 0.0512 0.0854 0.1200 0.308 0.0403 0.0406 0.0601
Accuracy was used to select the optimal model using the largest value. The final value used for the model was cp = 0.0394.
> print(modFit$finalModel, digits=3)
n= 2946
node), split, n, loss, yval, (yprob) * denotes terminal node
1) root 2946 2110 A (0.28 0.19 0.17 0.16 0.18) 2) roll_belt< 130 2665 1840 A (0.31 0.21 0.19 0.18 0.1) 4) pitch_forearm< -34.4 242 1 A (1 0.0041 0 0 0) * 5) pitch_forearm>=-34.4 2423 1840 A (0.24 0.23 0.21 0.2 0.11) 10) magnet_dumbbell_y< 440 2091 1520 A (0.27 0.19 0.24 0.19 0.11) 20) roll_forearm< 122 1287 788 A (0.39 0.19 0.2 0.16 0.064) * 21) roll_forearm>=122 804 550 C (0.085 0.19 0.32 0.23 0.18) * 11) magnet_dumbbell_y>=440 332 160 B (0.045 0.52 0.024 0.26 0.15) * 3) roll_belt>=130 281 14 E (0.05 0 0 0 0.95) * Data Analysis and Prediction for Test Data Train on training set 1 of 4 with no additional items using “Random Forest”. Run against testing set 1 of 4 with no extra features. {r}predictions <- predict(modFit, newdata=mylesstesting1) print(confusionMatrix(predictions, mylesstesting1$classe), digits=4) {r}predictions <- predict(modFit, newdata=mylesstesting1) print(confusionMatrix(predictions, mylesstesting1$classe), digits=4) Confusion Matrix and Statistics
Reference Prediction A B C D E A 503 152 170 137 46 B 7 136 15 68 57 C 45 92 157 116 86 D 0 0 0 0 0 E 3 0 0 0 171
Overall Statistics
Accuracy : 0.4931 95% CI : (0.4708, 0.5155) No Information Rate : 0.2845 P-Value [Acc > NIR] : < 2.2e-16
Kappa : 0.3377 Mcnemar's Test P-Value : NA
Statistics by Class:
Class: A Class: B Class: C Class: D Class: E Sensitivity 0.9014 0.35789 0.45906 0.0000 0.47500 Specificity 0.6401 0.90702 0.79061 1.0000 0.99813 Pos Pred Value 0.4990 0.48057 0.31653 NaN 0.98276 Neg Pred Value 0.9423 0.85459 0.87372 0.8363 0.89424 Prevalence 0.2845 0.19378 0.17440 0.1637 0.18358 Detection Rate 0.2565 0.06935 0.08006 0.0000 0.08720 Detection Prevalence 0.5140 0.14431 0.25293 0.0000 0.08873 Balanced Accuracy 0.7707 0.63246 0.62484 0.5000 0.73656 Run against 20 testing set provided by class instruction.
{r}print(predict(modFit, newdata=mytesting)) [1] C A C A A C C A A A C C C A C A A A A C Levels: A B C D E Estimation of model perfomance on validation data set Then, we estimate the performance of the model on the validation data set. Train on training set 4 of 4 with only cross validation. {r}set.seed(11355) modFit <- train(mylesstraining4$classe ~ ., method="rf", preProcess=c("center", "scale"), trControl=trainControl(method = "cv", number = 4), data=df_small_training4) print(modFit, digits=3) Run against 20 testing set provided in class. {r}print(predict(modFit, newdata=mytesting)) [1] C A C A A C C A A A C C C A C A A A A C Levels: A B C D E CART
2946 samples 53 predictor 5 classes: 'A', 'B', 'C', 'D', 'E'
No pre-processing Resampling: Bootstrapped (25 reps) Summary of sample sizes: 2946, 2946, 2946, 2946, 2946, 2946, ... Resampling results across tuning parameters:
cp Accuracy Kappa Accuracy SD Kappa SD 0.0308 0.549 0.4273 0.0370 0.0498 0.0422 0.500 0.3546 0.0510 0.0819 0.1181 0.317 0.0535 0.0419 0.0617
Accuracy was used to select the optimal model using the largest value. The final value used for the model was cp = 0.0308. Conclusion The out of sample error (error rate you get on new data set) in this analysis, it’s the error rate resulting after running the predict () function on the 4 testing sets. The testing set is approximately equal in size, thus in this analysis the random forest method was utilized in both preprocessing and cross validation against test sets 1-4 (set1:1 - .9714 = 0.0286, set21 - .9634 = 0.0366: set3:1 - .9655 = 0.0345, set4:1 - .9563 = 0.0437) yielding a predicted out of sample rate of 0.03585 when averaged. The analysis found three distinct predictions by appling the 4 models against the 20 item training set: A) Accuracy Rate 0.0286 Predictions: B A A A A E D B A A B C B A E E A B B B B) Accuracy Rates 0.0366 and 0.0345 Predictions: B A B A A E D B A A B C B A E E A B B B C) Accuracy Rate 0.0437 Predictions: B A B A A E D D A A B C B A E E A B B B Appendix:
I. Rattle FancyRpartPlot {r}fancyRpartPlot(modFit$finalModel) II. Decision Tree Visualization {r}treeModel <- rpart(classe ~ ., data=mylesstraining1, method="class") prp(treeModel)
Reference 1. Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human ’13). Stuttgart, Germany: ACM SIGCHI, 2013.
